# Highlight

A key component of transformers and LLMs is the self-attention mechanism

# Note


# CH 1 - P19\~P31

# 第一章：提示工程 (Prompt Engineering) 導論

### — 第19頁—

ChatGPT 於 2022 年 11 月下旬發布。到了隔年 1 月，該應用程式估計已累積了 1 億名月活躍用戶，這使得 ChatGPT 成為史上成長最快的消費者應用程式。（相比之下，TikTok 花了 9 個月才達到 1 億用戶，而 Instagram 則花了 2.5 年。）身為尊敬的讀者，您肯定可以證明，這種公眾的讚譽是實至名歸的！大型語言模型 (Large Language Models, LLMs)——就像支撐 ChatGPT 的那個模型——正在徹底改變我們的工作方式。現在，您不必再透過傳統的網路搜尋去 Google 找答案，而是可以輕鬆地要求 LLM 針對某個主題進行討論。您不必再為了回答技術問題而去閱讀 Stack Overflow 或翻閱部落格文章，而是可以要求 LLM 針對您的確切問題領域撰寫一份個人化教學，隨後針對該主題進行一連串的問答 (Q&A)。您不必再遵循傳統步驟來建立程式庫，而是可以透過與基於 LLM 的助手結對 (pairing) 來加速進度，讓助手在您編寫程式碼時為您搭建鷹架並自動補全程式碼！

而對於未來的讀者您來說，您使用 LLM 的方式，是否會是我們這些來自 2024 年、謙卑的作者們所無法想像的呢？如果目前的趨勢持續下去，您很可能在典型的一天中與 LLM 進行多次對話——當您的有線電視斷訊時，它會以 IT 支援助手的聲音出現；在街角的 ATM 上與您親切對話；甚至，是的，還包括那些令人沮喪卻逼真的自動撥號機器人。還會有其他的互動形式。LLM 將為您策劃新聞，總結您最可能感興趣的頭條故事，並移除（或可能增加）帶有偏見的評論。您將使用 LLM 來協助溝通，由它撰寫和總結電子郵件，而辦公室和家庭助手甚至會深入現實世界，代表您進行互動。在一天之內，您的個人 AI 助手可能在某個時刻扮演旅行社的角色，幫您制定旅遊計畫、預訂航班和旅館；而在另一個時刻，則扮演購物助手的角色，幫您尋找和購買您需要的物品。

### — 第20頁—

為什麼 LLM 如此令人驚嘆？因為它們就是魔法！正如未來學家 Arthur C. Clarke 的名言：「任何足夠先進的科技，都與魔法無異。」我們認為，一台您可以與之對話的機器肯定符合魔法的定義，但本書的目標就是要破除這種魔法。我們將證明，無論 LLM 有時看起來多麼不可思議、直觀且像人類，從本質上講，LLM 僅僅是預測一段文字中下一個字的各種模型——就只是這樣，別無其他！因此，LLM 只是幫助使用者完成某些任務的工具，而您與這些工具互動的方式，就是透過精心設計「提示 (prompt)」——即它們要補全的那段文字。這就是我們所說的提示工程 (Prompt Engineering)。透過本書，我們將建立一個提示工程的實用框架，並最終建立 LLM 應用程式，這將為您的使用者帶來如魔法般的體驗。

本章將為您即將展開的提示工程之旅奠定背景。但首先，讓我們告訴您，我們這兩位作者是如何親自發現這個魔法的。

## LLM 就是魔法

本書的兩位作者都是 GitHub Copilot 程式碼補全產品的早期研發人員。Albert 是創始團隊成員，而 John 則是在 Albert 轉向其他更長遠的 LLM 研究專案時加入的。

Albert 在 2020 年年中首次發現了這個魔法。他的描述如下：

> 每半年左右，在我們「程式碼機器學習 (ML-on-code)」小組的構思會議上，總會有人提出程式碼合成 (code synthesis) 的議題。而答案總是一樣的：總有一天它會很驚人，但那一天至少還要五年才會到來。它是我們的「冷核融合 (cold fusion)」。
>
> 這種情況一直持續到我第一次接觸到那個後來成為 OpenAI Codex 的 LLM 早期原型機的那一天。那時我明白未來已至：冷核融合終於實現了。
>
> 很明顯，這個模型與我們之前所知的那些糟糕的程式碼合成嘗試完全不同。這個模型不只是有機會預測下一個字——它可以僅憑文件字串 (docstring) 就生成完整的語句和完整的函式。而且是能運作的函式！
>
> 在我們決定可以用這個模型建立什麼之前（劇透：它最終成為了 GitHub 的 Copilot 程式碼補全產品），我們想要量化這個模型到底有多好。所以，我們群眾外包 (crowdsourced) 了一群 GitHub 工程師，讓他們想出一些獨立的程式設計任務。有些任務相對簡單——但這些都是硬核的程式設計師，他們的許多任務也相當複雜。有相當數量的任務是初級開發者會求助於 Google 的，但也有一些甚至會讓資深開發者不得不去查 Stack Overflow。然而，如果我們給模型幾次嘗試的機會，它就能解決其中的大部分問題。
>
> 那時我們就知道了——這就是將引領程式設計新時代的引擎。我們所要做的就是圍繞著它打造合適的載具。

### —第21頁—

對於 John 來說，這個神奇時刻發生在幾年後的 2023 年初，當時他正在測試這輛載具並試駕一番。他的敘述如下：

> 我設定了一個螢幕錄製工作階段，並列出了我打算解決的程式設計挑戰：建立一個函式，該函式接收一個整數並傳回該數字的文字版本。因此，如果輸入是 10，輸出就是 "ten"（十）；如果輸入是 1,004,712，輸出就是 "one million four thousand seven hundred twelve"（一百萬四千七百一十二）。這比你預期的要難，因為英語充滿了奇怪的例外。10 到 20 之間的數字文字版本——"eleven"（十一）、"twelve"（十二）以及十幾歲的數字 (teens)——並不遵循其他任何十進位數字的模式。十位數打破了預期的模式——例如，如果 90 是 "ninety"，80 是 "eighty"，那為什麼 30 不是 "threety"，20 不是 "twoty"？但我這個程式設計挑戰真正的轉折點在於，我想用一種我完全沒有個人經驗的語言來實作解決方案——Rust。Copilot 能應付這個挑戰嗎？
>
> 通常，在學習一種新的程式語言時，我會參考典型的入門指南：如何建立變數？如何建立清單 (list)？如何迭代清單中的項目？如何撰寫 if 語句？但在 Copilot 的協助下，我直接從寫一個文件字串 (docstring) 開始：

```rust
// GOAL: Create a function that prints a string version of any number supplied to the function.
// 1 -> "one"
// 2034 -> "two thousand thirty four"
// 11 ... "eleven"
fn
```

> Copilot 看到了 `fn` 就跳進來幫忙：

```rust
fn number_to_string(number: i32) -> String {
```

> 太完美了！我並不知道如何為函式的輸入參數或回傳值註解型別，但隨著我們繼續合作，我會透過註解來指導高層次的工作流程，像是「將輸入的數字分成三個一組 (Split up the input number into groups of three digits)」，而 Copilot 則有效地教會了我程式設計結構。這包括如何建立向量 (vectors) 並將其指派給變數，如 `let mut number_string_vec = Vec::new();`，以及如何建立迴圈，如 `while number > 0 {`。
>
> 這個體驗非常棒。我正在取得進展並學習語言，而沒有被不斷查閱語言教學所分心——我的專案就是我的教學。然後，在這個實驗進行了 20 分鐘後，Copilot 震撼了我。我輸入了一條註解，並開始了我認為我們需要的下一個控制迴圈：

```rust
// iterate through number_string_vec, assemble the name of the number
// for each order of magnitude, and concatenate to number_string for
```

### —第22頁—

> 停頓片刻後，Copilot 插入了 30 行程式碼！在錄影中，你實際上可以聽到我倒抽一口氣的聲音。程式碼編譯成功了——語法全部正確——而且可以執行。答案有點怪。輸入 5,034,012 得到的字串是 "five thirty four thousand twelve million"（五 三十四千 十二百萬），但是，嘿，我並不指望人類第一次就能做對，而且這個 bug 很容易發現並修正。在 40 分鐘的結對編程 (pairing session) 結束時，我完成了不可能的任務——我用一種我完全不熟悉的語言建立了非凡的程式碼！Copilot 指導我對 Rust 語法有了基本的了解，並且它展示了對我的目標更抽象的掌握，並在幾個點上介入幫助我填補細節。如果我自己嘗試，我懷疑這要花上好幾個小時。

我們神奇的經歷並非特例。如果您正在閱讀本書，您可能自己也與 LLM有過一些令人震撼的互動。或許您是在使用 ChatGPT 時首次意識到 LLM 的力量，或者您的第一次體驗是來自 2023 年初以來不斷湧現的第一代應用程式：像是微軟 Bing 或 Google Bard 這樣的網路搜尋助手，或是微軟更廣泛的 Copilot 套件等文件助手。但到達這個技術轉折點並非一夕之間發生的。要真正理解 LLM，了解我們是如何走到這一步是很重要的。

## 語言模型：我們是如何走到這一步的？

要了解我們是如何走到科技史上這個非常有趣的時刻，我們首先需要知道語言模型究竟是什麼以及它在做什麼。還有誰比世界上最受歡迎的 LLM 應用程式 ChatGPT 更適合回答這個問題呢？（見圖 1-1）。

![⏰\_書籍\_Books.png.jpg](./CH%201%20-%20P19~P31-assets/⏰_書籍_Books.png.jpg)

***You:** 什麼是語言模型？*

***ChatGPT:** 語言模型是一種人工智慧系統，經過訓練可以理解和生成類似人類的文字。它透過處理大量的文本數據來學習語言的結構、語法和語義。語言模型的主要目標是預測在給定上下文中下一個單詞或單詞序列的機率。*

### —第23頁—

看到了嗎？這就像我們在本章開頭所說的那樣：語言模型的主要目標是預測下一個字的機率。您以前見過這種功能，不是嗎？當您在 iPhone 上輸入簡訊時，鍵盤上方出現的補全詞條就是它（見圖 1-2）。您可能從未注意過它……因為它並不是那麼有用。如果這就是語言模型所做的一切，那它們究竟是如何席捲全球的呢？

![⏰\_書籍\_Books.png.jpg](./CH%201%20-%20P19~P31-assets/⏰_書籍_Books.png.jpg)

### 早期的語言模型

語言模型其實已經存在很長一段時間了。如果您在本書出版後不久閱讀，那麼支援 iPhone「猜測下一個字」功能的語言模型是基於自然語言的馬可夫模型 (Markov model)，該模型早在 1948 年就首次被引入。然而，還有其他更近代的語言模型，更直接地為正在進行的 AI 革命奠定了基礎。

到了 2014 年，最強大的語言模型是基於 Google 推出的序列對序列 (sequence to sequence, seq2seq) 架構。Seq2seq 是一種循環神經網路 (recurrent neural network)，理論上它應該非常適合文字處理，因為它一次處理一個標記 (token) 並循環更新其內部狀態。這使得 seq2seq 能夠處理任意長度的文字序列。透過專門的架構和訓練，seq2seq 架構能夠執行多種不同類型的自然語言任務：分類、實體擷取、翻譯、摘要等等。但這些模型有一個致命弱點——資訊瓶頸 (information bottleneck) 限制了它們的能力。

### —第24頁—

Seq2seq 架構有兩個主要組成部分：編碼器 (encoder) 和解碼器 (decoder)（見圖 1-3）。處理過程始於將一串標記 (tokens) 傳送給編碼器，一次處理一個。當接收到標記時，編碼器會更新一個隱藏狀態向量 (hidden state vector)，該向量累積了來自輸入序列的資訊。當最後一個標記處理完畢後，隱藏狀態的最終值，稱為思維向量 (thought vector)，會被傳送給解碼器。然後解碼器利用思維向量中的資訊來生成輸出標記。然而，問題在於思維向量是固定且有限的。它經常「忘記」長篇文字中的重要資訊，導致解碼器能利用的資訊很少——這就是資訊瓶頸。

圖中的模型運作如下：

1. 來源語言的標記被逐一傳送至編碼器並轉換為嵌入向量 (embedding vector)，並更新編碼器的內部狀態。

2. 內部狀態被打包成思維向量並傳送給解碼器。

3. 一個特殊的「開始」標記被傳送給解碼器，表示這是輸出標記的開始。

4. 以思維向量的值為條件，解碼器狀態被更新，並發出一個目標語言的輸出標記。

5. 輸出標記被作為下一個輸入提供給解碼器。此時，過程會在步驟 4 和步驟 5 之間循環往復。

6. 最後，解碼器發出一個特殊的「結束」標記，表示解碼過程完成。有限的思維向量只能傳遞有限的資訊量給解碼器。

2015 年的一篇論文《Neural Machine Translation by Jointly Learning to Align and Translate》（透過聯合學習對齊和翻譯的神經機器翻譯）引入了一種新方法來解決這個瓶頸。它不再讓編碼器提供單一的思維向量，而是保留編碼過程中為每個標記生成的所有隱藏狀態向量，然後允許解碼器對所有向量進行「軟搜尋 (soft search)」。作為示範，該論文顯示，在英法翻譯模型中使用軟搜尋顯著提高了翻譯品質。這種軟搜尋技術很快就被稱為注意力機制 (attention mechanism)。

注意力機制很快在 AI 社群獲得了大量的關注，最終導致了 2017 年 Google Research 的論文《Attention Is All You Need》（注意力就是你所需要的一切），該論文介紹了 Transformer 架構（見圖 1-4）。Transformer 保留了其前身的高層結構——包含接收標記作為輸入的編碼器，隨後是生成輸出標記的解碼器。但與 seq2seq 模型不同的是，所有的循環電路都被移除了，Transformer 完全依賴注意力機制。由此產生的架構非常靈活，並且在模擬訓練數據方面比 seq2seq 好得多。但是，seq2seq 可以處理任意長度的序列，而 Transformer 只能處理固定、有限的輸入和輸出序列。由於 Transformer 是 GPT 模型的直系祖先，這一直是我們努力克服的限制。

### GPT 登場

生成式預訓練 Transformer (Generative Pre-trained Transformer) 架構是在 2018 年的論文《Improving Language Understanding by Generative Pre-Training》（透過生成式預訓練改善語言理解）中引入的。這個架構並不是特別特殊或新穎。實際上，該架構只是一個被剝離了編碼器的 Transformer——它只是解碼器那一側。然而，這種簡化帶來了一些意想不到的新可能性，這些可能性在接下來的幾年才完全實現。正是這種生成式預訓練 Transformer 架構——GPT——即將點燃正在進行的 AI 革命。

在 2018 年，這點並不明顯。當時，標準做法是用未標記的數據（例如來自網際網路的文字片段）預訓練模型，然後修改模型的架構並應用專門的微調 (fine-tuning)，以便最終模型能夠非常好地執行一項任務。生成式預訓練 Transformer 架構也是如此。2018 年的論文僅表明這種模式對 GPT 非常有效——在未標記的文字上進行預訓練，隨後針對特定任務進行監督式微調，可以為各種任務（如分類、測量文件相似度和回答多選題）帶來非常好的模型。但我們應該強調一點：在 GPT 被微調之後，它只擅長它被微調的那一項任務。

GPT-2 只是 GPT 的放大版。當它在 2019 年推出時，研究人員開始意識到 GPT 架構有些特別。這在 OpenAI 介紹 GPT-2 的部落格文章的第二段中得到了清楚的證明：

> 我們的模型稱為 GPT-2（GPT 的繼任者），僅僅被訓練來預測 40 GB 網際網路文字中的下一個字。由於我們擔心該技術的惡意應用，我們不會發布訓練好的模型。

哇！這兩句話怎麼會放在一起？像預測下一個字這樣無害的事情——就像你在寫簡訊時 iPhone 做的那樣——怎麼會引起對誤用的如此嚴重擔憂？如果你閱讀相應的學術論文《Language Models Are Unsupervised Multitask Learners》（語言模型是非監督式多任務學習者），你就會開始明白了。GPT-2 有 15 億個參數 (parameters)，相較於 GPT 的 1.17 億個，並且在 40 GB 的文字上進行訓練，相較於 GPT 的 4.5 GB。模型和訓練集大小僅僅是一個數量級的增加，就導致了一種前所未有的湧現特質——你不必為單一任務微調 GPT-2，你可以將原始的、預訓練的模型應用於該任務，而且通常能獲得比專門為該任務微調的最先進模型更好的結果。這包括理解模棱兩可的代名詞、預測文字中缺失的單字、標記詞性 (parts of speech) 等基準測試。儘管落後於最先進的技術，GPT-2 在閱讀理解、摘要、翻譯和問答任務上的表現也出奇地好，再次超越了專門為這些任務微調的模型。

但是，為什麼要對這個模型的「惡意應用」如此擔憂呢？這是因為該模型已經變得非常擅長模仿自然文字。而且，正如 OpenAI 的部落格文章指出的那樣，這種能力可以用來「生成誤導性的新聞文章、在網路上冒充他人、自動生產辱罵性或虛假的內容發佈在社交媒體上，以及自動生產垃圾郵件/網路釣魚內容。」如果有什麼不同的話，這種可能性在今天比在 2019 年變得更加真實和令人擔憂。

GPT-3 在模型大小和訓練數據方面又增加了一個數量級，能力也隨之飛躍。2020 年的論文《Language Models Are Few-Shot Learners》（語言模型是少樣本學習者）表明，只要給出你想讓模型完成的任務的幾個範例（即「少樣本 (few-shot) 範例」），模型就能忠實地複製輸入模式，並因此執行幾乎任何你能想像到的基於語言的任務——而且通常能獲得非常高品質的結果。這就是我們發現你可以修改輸入——即提示 (prompt)——從而調節模型來執行手頭必要任務的時候。這就是提示工程的誕生。

ChatGPT 於 2022 年 11 月發布，由 GPT-3.5 支援——剩下的就是歷史了！但是，這是一段正在迅速創造的歷史（見表 1-1）。2023 年 3 月，GPT-4 發布，儘管細節沒有正式透露，但據傳該模型在模型大小和訓練數據量方面又是一個數量級的增長，並且再次比其前輩強大得多。從那時起，越來越多的模型出現了。有些來自 OpenAI，有些則來自主要的產業參與者，如 Meta 的 Llama、Anthropic 的 Claude 和 Google 的 Gemini。我們繼續看到品質的飛躍，而且同樣品質水準的模型正日益出現在更小、更快的模型中。如果有什麼不同的話，那就是進步只會加速。

**表 1-1. GPT 系列模型的詳細資訊，顯示所有指標呈指數級增長**

| 模型 | 發布日期 | 參數數量 | 訓練數據 | 訓練成本 | 
|---|---|---|---|---|
| GPT-1 | 2018 年 6 月 11 日 | 1\.17 億 | BookCorpus：來自 7,000 本各種類型未出版書籍的 4.5 GB 文字 | 1\.7e19 FLOP | 
| GPT-2 | 2019 年 2 月 14 日（初始）；2019 年 11 月 5 日（完整） | 15 億 | WebText：來自 Reddit 上被推崇的 4500 萬個網頁的 40 GB 文字和 800 萬份文件 | 1\.5e21 FLOP | 
| GPT-3 | 2020 年 5 月 28 日 | 1750 億 | 4990 億個 token，包括 Common Crawl (570 GB)、WebText、英文維基百科和兩個書籍語料庫 (Books1 和 Books2) | 3\.1e23 FLOP | 
| GPT-3.5 | 2022 年 3 月 15 日 | 1750 億 | 未公開 | 未公開 | 
| GPT-4 | 2023 年 3 月 14 日 | 1\.8 兆（傳聞） | 據傳為 13 兆個 token | 估計為 2.1e25 FLOP | 

## 提示工程 (Prompt Engineering)

現在，我們來到了您進入提示工程世界之旅的起點。從核心來看，LLM 只能做一件事——補全文字。輸入到模型的內容稱為提示 (prompt)——它是我們期望模型去補全的一份文件或一段文字塊。那麼，最簡單形式的提示工程，就是精心製作提示，使其補全內容包含解決手頭問題所需的資訊。

在本書中，我們提供了提示工程的更宏觀圖景，其涉及的遠不止單一提示，並討論了整個基於 LLM 的應用程式，其中提示的建構和答案的解釋是透過程式化完成的。要建立高品質的軟體和高品質的使用者體驗 (UX)，提示工程師必須為使用者、應用程式和 LLM 之間的反覆溝通建立一種模式。使用者將問題傳達給應用程式，應用程式建構一份偽文件 (pseudodocument) 傳送給 LLM，LLM 補全該文件，最後，應用程式解析補全內容並將結果傳回給使用者，或代表使用者執行動作。提示工程的科學與藝術在於確保這種溝通以最佳方式在截然不同的領域（使用者的問題空間和 LLM 的文件空間）之間進行轉換。

提示工程有幾個複雜程度等級。最基本的形式僅使用非常薄的應用程式層。例如，當您與 ChatGPT 互動時，您幾乎是在直接撰寫提示；應用程式僅僅是將對話串包裝在一種特殊的 ChatML markdown 中。（您將在第 3 章了解更多相關資訊。）同樣地，當 GitHub Copilot 最初被建立用於程式碼補全時，它所做的僅僅是將目前的檔案傳遞給模型進行補全。

在下一個複雜程度等級，提示工程涉及修改和增強使用者對模型的輸入。例如，LLM 處理的是文字，因此技術支援熱線可以將使用者的語音轉錄為文字，並將其用於傳送給 LLM 的提示中。此外，來自先前幫助記錄的相關內容或相關支援文件也可以包含在提示中。舉一個現實世界的例子，隨著 GitHub Copilot 程式碼補全的發展，我們意識到如果我們納入使用者相鄰分頁中的相關片段，補全品質會顯著提高。這是有道理的，對吧？使用者打開這些分頁是因為他們正在參考那裡的資訊，所以理所當然地，模型也可以從這些資訊中受益。另一個例子是新的 Bing 聊天式搜尋體驗。在這種情況下，來自傳統搜尋結果的內容被拉入提示中。這使得助手能夠勝任地討論它在訓練數據中從未見過的資訊（例如，因為它指的是模型訓練後發生的事件）。更重要的是，這種方法有助於 Bing 減少幻覺 (hallucinations)，我們將在本書中多次重溫這個主題，從下一章開始。

這個複雜程度等級的提示工程的另一個面向出現在與 LLM 的互動變為有狀態 (stateful) 時，這意味著它們保持了先前互動的上下文和資訊。聊天應用程式就是這裡最典型的例子。隨著使用者的每一次新交流，應用程式必須回憶先前的交流中發生了什麼，並生成一個忠實代表該互動的提示。隨著對話或歷史記錄變長，您必須小心不要填滿提示或包含可能分散模型注意力的虛假內容。您可能會選擇捨棄最早的交流或先前交流中較不相關的內容，甚至可能採用摘要 (summarization) 來壓縮內容。

這個複雜程度等級的提示工程還有另一個面向，涉及為基於 LLM 的應用程式提供工具，允許 LLM 透過發出 API 請求來讀取資訊，甚至建立或修改網際網路上的資產，從而接觸現實世界。例如，一個基於 LLM 的電子郵件應用程式可能會收到使用者的輸入：「寄給 Diane 一封 5 月 5 日會議的邀請函。」這個應用程式會使用一個工具在使用者的聯絡人清單中識別 Diane，然後使用行事曆 API 查詢她的空閒時間，最後發送電子郵件邀請。隨著這些模型變得更便宜、更強大，想像一下今天我們手邊已有的 API 所帶來的可能性！在這裡，提示工程至關重要。模型將如何知道使用哪個工具？它將如何以正確的方式使用工具？您的應用程式將如何正確地與模型分享工具執行的資訊？當工具使用導致某種錯誤狀態時，我們該怎麼辦？我們將在第 8 章討論所有這些內容。

我們在本書中涵蓋的最後一個複雜程度等級是如何賦予 LLM 應用程式代理權 (agency)——即能夠自行決定如何完成使用者提供的廣泛目標的能力。這顯然是我們 LLM 能力的前沿，但研究和實際探索正在進行中。現在，您已經可以下載 AutoGPT 並提供給它一個目標，它就會啟動一個多步驟流程來收集完成目標所需的資訊。它總是有效嗎？不。實際上，除非目標受到相當的限制，否則它執行任務失敗的次數往往多於成功的次數。但是，賦予 LLM 應用程式某種形式的代理權和自主權仍然是邁向令人興奮的未來可能性的重要一步。您將在第 8 章和第 9 章閱讀我們對此的看法。

## 結論

正如我們一開始所說，本章為您即將展開的提示工程之旅奠定了背景。我們首先討論了語言模型的近期歷史，並強調了為什麼 LLM 如此特別和不同——以及為什麼它們正在推動我們現在都在見證的 AI 革命。然後我們定義了本書的主題：提示工程。

特別是，您應該了解本書不會全是關於如何挑剔單一提示的措辭以獲得一個好的補全。當然，我們會涵蓋這一點，我們會詳細涵蓋您生成符合預期目的的高品質補全所需做的所有事情。但是當我們說「提示工程」時，我們指的是建立整個基於 LLM 的應用程式。LLM 應用程式充當轉換層，反覆且有狀態地將現實世界的需求轉換為 LLM 可以處理的文字，然後將 LLM 提供的數據轉換為解決這些現實世界需求的資訊和行動。

在我們踏上這段旅程之前，讓我們確保我們已經做好了適當的打包。在下一章，您將學習 LLM 文字補全是如何運作的，從最上層的 API 一直到低層的注意力機制。在隨後的章節中，我們將基於該知識來解釋 LLM 如何擴展到處理聊天和工具使用，您會看到在深層次上，這其實都是同一件事——文字補全。然後，有了這些基礎概念，您就準備好開始您的旅程了。

# 第二章：理解 LLM (Understanding LLMs)

所以您想成為 LLM 溝通者 (LLM whisperer)，透過巧妙的提示解鎖它們豐富的知識和處理能力？好吧，要欣賞哪種提示是巧妙的並能從 LLM 引出正確答案，您首先需要了解 LLM 如何處理資訊——它們如何思考。

在本章中，我們將像剝洋蔥一樣探討這個問題。首先，您將在「什麼是 LLM？」一節（第 16 頁）從最外層看到 LLM 作為受過訓練的文字模仿者。您將在「LLM 如何看待世界」一節（第 22 頁）了解它們如何將文字分割成稱為詞元 (tokens) 的一口大小的塊狀物，並且您將了解如果它們無法輕易完成這種分割會產生什麼後果。

您還將在「一次一個 Token」一節（第 29 頁）發現 token 序列是如何一點一點生成的，並在「溫度與機率」一節（第 32 頁）了解選擇下一個 token 的不同方法。最後，在「Transformer 架構」一節（第 37 頁），您將深入探究 LLM 的內部運作，將其理解為透過稱為注意力 (attention) 的問答遊戲進行溝通的微型大腦集合，並了解這對提示順序意味著什麼。

在所有這些過程中，請記住這是一本關於「使用」LLM 的書，而不是關於 LLM 本身的書。因此，有很多很酷的技術細節我們沒有提到，因為它們與提示工程無關。如果您想要矩陣乘法和激活函數 (activation functions)，您需要轉向別處——經典參考資料《The Illustrated Transformer》（圖解 Transformer）是深入研究的絕佳起點。但我們保證，如果您只想寫出很棒的提示，您不需要那麼多的技術背景——所以讓我們深入了解您確實需要知道的內容。

## 什麼是 LLM？

在最基本的層面上，LLM 是一個接收字串並傳回字串的服務：文字進，文字出。輸入稱為提示 (prompt)，輸出稱為補全 (completion) 或有時稱為回應 (response)（見圖 2-1）。

當一個未經訓練的 LLM 首次問世時，它的補全看起來會像是一堆相當隨機的 unicode 符號，與提示沒有明確的關係。它需要經過訓練才有用。那時，LLM 將不再只是用字串回答字串，而是用語言回答語言。

訓練需要大量的技巧、運算能力和時間，遠超出大多數專案小組的範圍，因此大多數 LLM 應用程式使用現成的通才模型（稱為基礎模型 foundation models），這些模型已經過訓練（可能經過了一點微調；見側邊欄）。所以，我們不指望您自己訓練 LLM——但如果您想使用 LLM，特別是透過程式化方式，了解它被訓練來做什麼是至關重要的。

### 什麼是微調 (Fine-Tuning)？

訓練 LLM 需要大量的數據和運算，儘管許多基本課程，如英語語法規則，在訓練集之間並沒有太大差異。因此，訓練 LLM 時通常不會完全從頭開始，而是從另一個 LLM 的副本開始，可能是一個在不同文件上訓練過的模型。

例如，OpenAI Codex（為 GitHub Copilot 開發的用於生成原始碼的 LLM）的早期版本是現有模型（GPT-3，一種自然語言 LLM）的副本，並使用 GitHub 上發布的大量原始碼進行了微調。

如果您有一個在數據集 A 上訓練並在數據集 B 上微調的模型，您的提示通常應該寫得好像它是直接在 B 上訓練的一樣。我們將在第 7 章更深入地探討微調。

LLM 使用大量稱為訓練集 (training set) 的文件（同樣是字串）進行訓練。文件的種類取決於 LLM 的用途（範例見圖 2-2）。訓練集通常是不同訓練輸入的混合物，如書籍、文章、Reddit 等平台上的對話以及 GitHub 等網站上的程式碼。模型應該從訓練集中學習如何產生看起來就像訓練集的輸出。具體來說，當模型收到一個提示，該提示是其訓練集中某份文件的開頭時，產生的補全應該是最可能延續原始文件的文字。換句話說，模型會模仿。

那麼，LLM 與一個充滿訓練數據的大型搜尋引擎索引有什麼不同呢？畢竟，搜尋引擎也能完美完成 LLM 被訓練的任務——給定一份文件的開頭，它可以以 100% 的準確度找到該文件的補全。然而，擁有一個只是鸚鵡學舌般複述訓練集的搜尋引擎並不是這裡的目標：LLM 不應該死記硬背訓練集，而是應用它在那裡遇到的模式（特別是邏輯和推理模式）來補全任何提示，而不僅僅是來自訓練集的提示。單純的死記硬背被認為是一種缺陷。LLM 的內部架構（鼓勵它從具體範例中抽象化）和訓練程序（試圖提供多樣化、非重複的數據並測量在未見過數據上的成功）都旨在防止這種缺陷。

這種預防有時會失敗，模型非但沒有學習事實和模式，反而死記硬背了大段文字——這稱為過度擬合 (overfitting)。在現成的模型中，大規模的過度擬合應該很少見，但值得注意的是，如果 LLM 似乎解決了它在訓練期間見過的問題，這並不一定意味著當遇到它以前沒見過的類似問題時，LLM 也會做得一樣好。

儘管如此，在您使用 LLM 一段時間後，您開始會根據它所受訓練的任務，對 LLM 的行為產生直覺。因此，當您想知道某個提示可能會如何被補全時，不要問自己一個理智的人會如何「回覆」該提示，而是問一份恰好以該提示開頭的文件可能會如何繼續。

假設您隨機從訓練集中挑選了一份文件。您對它的唯一了解是，它以該提示開頭。統計上最可能的後續內容是什麼？那就是您應該預期的 LLM 輸出。

## 補全一份文件

這裡有一個關於推理文件補全的例子。考慮以下文字：

`Yesterday, my TV stopped working. Now, I can’t turn it on at`（昨天，我的電視壞了。現在，我根本無法將其打...）

對於以這樣開頭的文字，統計上最可能的補全可能是什麼？

1. `y2ior3w`

2. `Thursday.`

3. `all.`（開。）

這些補全沒有一個是絕對不可能的。有時，一隻貓跑過鍵盤，生成了補全 1；其他時候，句子在重寫時變得混亂，出現了 2。但目前為止最可能的後續是 3，幾乎所有的 LLM 都會選擇這個後續。

讓我們將補全 3 視為已知，並讓 LLM 再跑遠一點：

`Yesterday, my TV stopped working. Now, I can’t turn it on at all.`（昨天，我的電視壞了。現在，我根本無法將其打開。）

對於以那樣開頭的文字，統計上最可能的補全是什麼？

a. `This is why I chose to settle down with a book tonight.`（這就是為什麼我今晚選擇靜下心來看書。） b. `Shall we watch the game at your place instead?`（我們改去你家看比賽好嗎？） c. `\n \n First, try unplugging the TV from the wall and plugging it back in.`（\\n \\n 首先，嘗試將電視插頭從牆上拔下再插回去。）

嗯，這取決於訓練集。比方說 LLM 是在敘事散文（如短篇小說、長篇小說、雜誌和報紙）的數據集上訓練的——在這種情況下，關於看書的補全 a 聽起來比其他的更有可能。雖然關於電視的句子，後面接補全 b 的問題，很可能出現在故事中間的某個地方，但一個故事不會在沒有至少加上起始引號 (“) 的情況下以這個問題開場。所以一個在短篇小說上訓練的模型不太可能預測選項 b。

但是，把電子郵件和對話紀錄丟進訓練集，突然間，選項 b 顯得非常合理。不過，這兩個都是我編造的：其實是第三個選項由一個真實的 LLM（OpenAI 的 text-davinci-003，GPT-3 的一個變體）產生，模仿了其訓練集中大量的建議和客戶服務對話。

這裡浮現了一個主題：您越了解訓練數據，就越能對在該訓練數據上訓練的 LLM 的可能輸出形成良好的直覺。許多商業 LLM 不會公開它們的訓練數據——選擇一個好的訓練集是使其模型成功的獨門秘方的一大部分。然而，即使在那時，通常也可以對訓練集構成的文件類型形成一些合理的預期。

## 人類思維與 LLM 處理

LLM 選擇看起來最可能的後續內容，這與人類閱讀文字時的一些假設相違背。這是因為當人類產生文字時，他們是作為一個過程的一部分來進行的，這個過程不僅僅是產生看起來合理的文字輸出。比方說您想寫一篇關於您在 podcast 網站 Acast 上偶然發現的 podcast 的部落格文章。您可能開始寫道：`In their newest installment of 'The rest is history', they talk about the Hundred Years’ War (listen on acast at http://`（在他們最新一期的『The rest is history』中，他們談論了百年戰爭（在 acast 收聽，網址是 http://）。當然您不會背得網址，所以這是您停止寫作並進行快速網路搜尋的時間點。希望您能找到正確的連結：`[shows.acast.com/the-rest-is-history-podcast/episodes/321-hundred-years-war-a-storm-of-swords](shows.acast.com/the-rest-is-history-podcast/episodes/321-hundred-years-war-a-storm-of-swords)`。或者也許您找不到，在這種情況下，您可能會回去刪除整個括號並將其替換為 `(episode unfortunately not available anymore)`（很遺憾該集已不再提供）。

模型不能使用 Google 或編輯，所以它只能猜測。原始 LLM 也不會表達任何懷疑，添加免責聲明說它只是在猜測，或顯示任何其他證據表明該資訊僅僅是猜測而非實際知識——因為畢竟，模型總是在猜測。這個猜測恰好是在人類通常會切換到另一種產生文字模式（使用 Google 而不是按下腦海中浮現的第一個按鍵）的時間點做出的。

LLM 非常擅長模仿它們在猜測項目中發現的任何模式。畢竟，這幾乎就是它們被訓練來做的事情。所以如果它們編造一個社會安全號碼，那將是一串看起來合理的數字；如果它們編造一個 podcast 的網址，那看起來就像一個 podcast 的網址。

在這個案例中，我嘗試了 OpenAI 的 text-curie-001，這是 GPT-3 的一個小型變體，這個 LLM 將網址補全如下：

`<http://www.acast.com/the-rest-is-history-episode-5-the-Hundred-Years-War-\1411-1453-with-dr-martin-kemp>``)`

這裡的 Dr. Martin Kemp 是真實的人嗎？也許是一個參與歷史 podcast 的人？甚至可能是我們正在談論的那個 podcast？牛津大學確實有一位名叫 Martin Kemp 的藝術史學家，儘管補全內容是否指的是他，聽起來像是一個語言理論問題而不是 LLM 問題（見圖 2-3）。無論如何，他並沒有在《The Rest Is History》podcast 上談論百年戰爭。

### 幻覺 (Hallucinations)

LLM 被訓練為「訓練數據模仿機器」的事實帶來了不幸的後果：幻覺 (hallucinations)，即由模型自信地產生但在事實上錯誤卻看起來合理的資訊片段。這是使用 LLM 時常見的問題，無論是臨時使用還是在應用程式中使用。

由於從模型的角度來看，幻覺與其他補全沒有區別，因此像「不要捏造事實」這樣的提示指令用處非常有限。相反，典型的方法是讓模型提供一些可以查核的背景資訊。這可以是其推理的解釋、可以獨立執行的計算、來源連結，或是可以搜尋的關鍵字和細節。例如，要查核「有一位英國國王娶了他的表妹」這句話，比查核「有一位英國國王娶了他的表妹，即喬治四世，他娶了布藍茲維的卡羅琳」要困難得多。對抗幻覺的最佳解藥是「信任但要查證 (Trust but verify)」，只是要去掉信任。

幻覺也可以被誘導。如果您的提示引用了不存在的東西，LLM 通常會繼續假設它的存在。一開始提出錯誤主張然後在半途自我糾正的文件很少見。所以模型通常會假設其提示是真實的，這被稱為真相偏差 (truth bias)。

您可以利用真相偏差為您服務——如果您希望模型評估一個假設或反事實的情況，不需要說：「假裝現在是 2030 年，尼安德塔人已經復活了。」只需以「現在是 2031 年，距離第一批尼安德塔人復活已經整整一年了」開頭。

如果您可以使用產生補全的 LLM（即原始 LLM，而非包裹在像 ChatGPT 這樣的聊天介面中），這可能是一個嘗試輸入幾個所謂的「假裝 (make-believe) 提示」的好時機。

> 就像前面關於復活尼安德塔人的例子一樣，假裝提示不是透過直接提問來引出對假設性問題的回答，而是透過暗示假設性場景實際上已經發生來引出。

將建議與聊天 LLM 的回答進行比較。它有什麼不同？

然而，LLM 的真相偏差也是危險的，特別是對程式化應用程式而言。在程式化提示建立中，很容易搞砸並引入反事實或荒謬的元素。人類可能會讀完提示，放下紙張，對你挑起眉毛，然後說：「真的嗎？」LLM 沒有這個選項。它會盡最大努力假裝提示是真實的，而且它不太可能糾正你。所以您有責任給它一個不需要糾正的提示。

## LLM 如何看待世界

在第 16 頁的「什麼是 LLM？」中，您了解到 LLM 消耗並產生字串。值得深入了解這句話的含義：LLM 如何看待字串？我們習慣將字串視為字元 (characters) 序列，但這並不完全是 LLM 所看到的。它可以推理字元，但那不是原生能力，而且需要 LLM 相當於進行深度專注——在撰寫本文時（2024 年秋季），即使是最先進的模型仍然可能被像「'strawberry' 中有幾個 R？」這樣的問題所愚弄。

也許值得指出的是，我們其實也不是以字元來閱讀字串的。在人類處理的非常早期階段，它們就被組合成單詞 (words)。我們隨後閱讀的是單詞，而不是字母。這就是為什麼我們經常用閱讀帶過錯字而沒有發現它們：當它們到達我們處理的意識部分時，大腦已經修正了它們。

您可以透過故意打亂句子，使其剛好處於您內部自動修正功能可以應付的邊緣，來獲得很多樂趣（見圖 2-4，左）。然而，如果您以不尊重單詞邊界的方式打亂文字，您的讀者將會過得很慘（見圖 2-4，右）。

圖的左邊部分保留了完整的單詞邊界，並打亂了每個單詞內的字母順序，而右邊部分保留了完整的字母順序，但改變了單詞邊界。大多數人發現左邊的變體明顯更容易閱讀。

像人類一樣，LLM 也不閱讀單個字母。當您發送一段文字給模型時，它首先被分解成一系列稱為標記 (tokens) 的多字母區塊。它們通常是三到四個字元長，但常見單詞或字母序列也有更長的 token。模型使用的一組 token 稱為其詞彙表 (vocabulary)。

在閱讀一段文字時，模型首先將其通過一個分詞器 (tokenizer)，將其轉換為 token 序列。只有這樣它才會被傳遞給 LLM 本體。然後，LLM 產生一系列 token（在內部表示為數字），在您取回之前再轉換回文字（見圖 2-5）。

> 請注意，並非所有的分詞器都包含以空格開頭的複合 token，但許多都有。著名的例子是 OpenAI 的分詞器。

LLM 將文字視為由 token 組成，而人類將其視為由單詞組成。這聽起來好像 LLM 和人類以非常相似的方式看待文字，但有幾個關鍵的區別。

### 區別 1：LLM 使用確定性分詞器 (Deterministic Tokenizers)

作為人類，我們將字母轉換為單詞是模糊的。我們試圖找到一個與我們看到的字母序列最相似的單詞。另一方面，LLM 使用確定性分詞器——這使得錯字像受傷的拇指一樣突兀。單詞 `ghost` 在 OpenAI 的 GPT 分詞器（一種廣泛使用的分詞器，不僅用於 OpenAI 的模型）中是一個單一 token。然而，錯字 "gohst" 被轉換為三個 token 的序列——`g-oh-st`——這明顯不同，這使得 LLM 很容易發現錯字。儘管如此，LLM 通常對錯字相當有彈性，因為它們從訓練集中習慣了這些錯字。

### 區別 2：LLM 無法慢下來檢查字母

我們人類可以慢下來並有意識地單獨檢查每個字母，但 LLM 只能使用其內建的分詞器（而且它也不能慢下來）。許多 LLM 已經從訓練集中學到了哪個 token 由哪些字母組成，但這使得所有需要模型拆分或重新組裝 token 的語法任務變得更加困難。

圖 2-6 中有一個很好的例子，描述了關於反轉單詞中字母的 ChatGPT 對話。反轉字母是一種簡單的模式操作，LLM 通常非常擅長。但是拆分和重新組裝 token 證明對 LLM 來說太難了，所以反轉和重新反轉都差得很遠。

在圖中，初始反轉和重新反轉都充滿了錯誤。作為應用程式建立者，這裡的收穫是避免給模型這種涉及子 token (subtoken) 層級的任務，如果可以的話。

> 如果您希望 LLM 執行的任務包含一個需要模型拆分和重新組裝 token 的組件，請考慮是否可以在前處理或後處理中處理該組件。

作為如何使用方框中提示的例子，比方說您的應用程式正在使用 LLM 玩像 Scattergories 這樣的遊戲，其中的目標是尋找具有語法屬性的範例，例如「以 W 開頭的禁酒令活動家」、「以 Sw 開頭的歐洲國家」或「名稱中有 3 個字母 R 的水果」。那麼，將 LLM 作為神諭 (oracle) 來獲取大量的禁酒令活動家或歐洲國家清單，然後使用語法邏輯來過濾該清單，對您來說可能是有意義的。如果您試圖讓 LLM 承擔全部負擔，您可能會遇到失敗（見圖 2-7）。

注意圖中的模型不是確定性的，它以兩種不同的方式失敗（見第一次和第二次嘗試）。還要注意，`[ Sweden]`、`[ Switzerland]` 和 `[ Somalia]` 在 ChatGPT 的分詞器中都是獨立的 token。

### 區別 3：LLM 以不同方式看待文字

我們想要強調的最後一個區別是，我們人類對 token 和字母的許多方面有直觀的理解。特別是，我們看得到它們，所以我們知道哪些字母是圓的，哪些是方的。我們理解 ASCII 藝術，因為我們看得到它（儘管許多模型已經背下了大量的 ASCII 藝術）。對我們來說，一個帶有重音符號的字母只是同一個字母的變體，而我們在閱讀充滿這些符號的文字時，幾乎不會有太大的困難去忽略它們 (we have no great díffícúlty ígnóríng thém whílé réádíng á téxt whéré théy ábóúnd)。另一方面，模型即使能夠應付，也不得不使用其大量的處理能力，留給您心目中實際應用程式的處理能力就變少了。

這裡的一個特殊情況是大寫 (capitalization)。參考圖 2-8。為什麼這個簡單的任務進行得如此...我是說...變得如此糟糕？記住分詞的陷阱，在您繼續閱讀之前，您可能會嘗試自己猜測一下。

這產生了一些有趣且典型的錯誤——請注意，我們使用的是一個非常小的模型進行演示，較大的模型通常不會像這樣輕易被抓包。

對於人類來說，大寫字母 A 只是人類小寫字母 a 的變體，但包含大寫字母的 token 與包含小寫字母的 token 非常不同。這是模型非常清楚的事情，因為它們已經看過大量關於這方面的訓練數據。它們知道句點後的 token `For` 與句子中間的 token `for` 非常相似。

然而，大多數分詞器並不容易讓模型學習這些連結，因為大寫 token 並不總是一對一地對應非大寫 token。例如，GPT 分詞器將 "strange new worlds" 轉換為 `[str][ange][ new][ worlds]`，這是四個 token。但在全大寫中，分詞變成了 `[STR][ANGE][ NEW][ WOR][L][DS]`，這是六個 token。同樣地，單詞 `gone` 是一個單一 token，而 `[G][ONE]` 是兩個。

更好的 LLM 更善於處理這些大寫問題，但這對它們來說仍然是工作，會分散解決您真正問題的注意力，而您的問題可能不是大寫。（畢竟您不需要 LLM 來將文字大寫！）因此，明智的提示工程師會試圖避免讓 LLM 總是翻譯不同的大小寫，從而過度增加模型的負擔。

## 計算 Token

您不能混合搭配分詞器和模型。每個模型都使用固定的分詞器，因此非常值得了解您的模型的分詞器。

在撰寫 LLM 應用程式時，您可能會想在提示工程期間運行分詞器，使用像 Hugging Face 或 `tiktoken` 這樣的函式庫。然而，您的分詞器最常見的應用將比複雜的 token 邊界分析更平凡。您最常使用分詞器僅僅是為了計數。

這是因為從模型的角度來看，token 的數量決定了您的文字有多長。這包括長度的所有方面：模型閱讀提示所需的時間大致與提示中的 token 數量成線性比例。同樣地，它建立解決方案所花費的時間與產生的 token 數量成線性比例。運算成本也是如此：預測需要多少運算能力與其長度成正比。這就是為什麼大多數模型即服務 (model-as-a-service) 的產品會按產生或處理的 token 收費。在撰寫本文時，一美元通常可以購買 50,000 到 1,000,000 個輸出 token，具體取決於模型。

最後，token 的數量對於上下文視窗 (context window) 的問題至關重要——上下文視窗是 LLM 在任何給定時間可以處理的文字量。這是所有現代 LLM 的限制，我們將在本書中一次又一次地重溫這一點。

LLM 不只是接收任何文字並產生任何文字。它接收的文字 token 數量小於上下文視窗大小，並且其補全必須使得提示加上補全的 token 總數也不超過上下文視窗大小。上下文視窗大小通常以數千個 token 來衡量，理論上這不容小覷：它是幾頁，通常是幾十頁，有時是幾百頁 A4 大小的內容。但實務上往往還是會對其嗤之以鼻：無論您的上下文視窗有多長，您都會想填滿它並溢出它，所以您需要計算 token 以防止這種情況發生。

沒有將字元數轉換為 token 數的通用公式。這取決於文字和分詞器。上面連結的非常常見的 GPT 分詞器在對英語自然語言文字進行分詞時，大約每 token 四個字元。這相當典型，儘管較新的分詞器可能稍微更有效率（即，平均而言，每個 token 可以有更多字元）。

大多數分詞器都是針對英語最佳化的，對於其他語言效率較低，這意味著它們每個 token 的字元數較少。隨機數字字串的效率甚至更低，大約每個 token 兩個字元多一點。對於像加密金鑰這樣的隨機英數字串來說情況更糟，通常每個 token 不到兩個字元。帶有罕見字元的字串每個 token 的字元數最少——例如，unicode 笑臉 $\circleddot$ 實際上是兩個 token。

大多數 LLM 使用至少包含幾個特殊 token 的詞彙表：最常見的是，至少有一個文本結束 (end-of-text) token，在訓練中，它被附加到每個訓練文件的末尾，以便模型學習何時結束。每當模型輸出該 token 時，補全就會在該點切斷。

## 一次一個 Token

讓我們再剝掉一層洋蔥——這是到達核心之前的最後一層。在底層，LLM 並不是直接文字對文字，其實也不直接是 token 對 token。它是多個 token 對單一 token。模型只是不斷重複操作以獲取下一個 token，並在需要時累積這些單一 token 以獲得適當的文字輸出。

### 自回歸模型 (Auto-Regressive Models)

通過 LLM 的單次傳遞會給您統計上最可能的下一個 token。然後，將此 token 貼到提示上，LLM 再次傳遞以獲取給定新提示下統計上最可能的下一個 token，依此類推（見圖 2-9）。這樣一個一次做出一個預測，且下一個預測取決於先前預測的過程，稱為自回歸 (autoregressive)。

您知道當您在手機上寫簡訊時，鍵盤上方會出現三個單詞建議嗎？運行 LLM 就像是反覆按下中間的按鈕。

這種每一步一個 token 的規律、幾乎單調的模式，指出了 LLM 生成文字與人類輸入文字之間的一個巨大差異：雖然我們可能會停下來檢查、思考或反思，但模型需要在每一步產生一個 token。如果需要思考更長時間，LLM 不會獲得額外的時間，它也不能停頓。

而且一旦它輸出了一個 token，LLM 就致力於該 token。LLM 不能回溯並刪除 token。它也不會發布更正，說明它之前輸出的內容是不正確的，因為它沒有在那些會明確收回錯誤的文件上進行訓練——畢竟，撰寫這些文件的人類可以在錯誤發生的地方回溯並更正錯誤，所以明確的「收回 (takebacks)」在完成的文件中非常罕見。喔等等，其實，takebacks 更常拼成兩個字，所以讓我寫成 explicit take backs（明確的收回）吧。

這種特質會讓 LLM 看起來很固執，甚至有些荒謬，當它們繼續探索一條顯然沒有意義的路徑時。但實際上，這意味著，必要時，這種錯誤識別和回溯能力需要由應用程式設計師（也就是您）來提供。

### 模式與重複

自回歸系統的另一個問題是它們可能會陷入自己的模式。LLM 擅長識別模式，所以它們有時會（偶然地）建立一個模式，並且找不到一個好的點離開它。畢竟，鑑於該模式，在任何給定的 token，它繼續下去的可能性都比中斷的可能性大。這導致了非常重複的解決方案（見圖 2-10）。

在圖中，一個 LLM 產生了一份喜歡某個電視節目的理由清單。您能發現多少個模式？以下是我們發現的：

- 這些項目是連續編號的語句，每個語句都適合放在一行。這似乎是可取的。

- 它們都以 "The" 開頭，這似乎可以容忍。

- 它們的形式是 $X$ is $Y$ and $Z$。這很煩人，因為它危及正確性。如果沒有合適的 Z 怎麼辦？模型可能會發明一個。不過，它在第 5 項之後停止了。

- 在連續幾個項目都以 "The franchise" 開頭後，全部都變成了這樣。這很蠢。

- 到了最後，legacy（遺產）、following（追隨者）、future（未來）、foundation（基礎）和 fanbase（粉絲群）被令人作嘔地重複。這也很蠢。

- 清單沒完沒了，從不停止。這是因為在每個項目之後，清單繼續的可能性比這將是最後一個項目的可能性更大。而且模型不會感到無聊。

- 到了最後，legacy（遺產）、following（追隨者）、future（未來）、foundation（基礎）和 fanbase（粉絲群）被令人作嘔地重複。這也很蠢。

- 清單沒完沒了，從不停止。這是因為在每個項目之後，清單繼續的可能性比這將是最後一個項目的可能性更大。而且模型不會感到無聊。

處理這種重複性解決方案的方法通常是簡單地檢測並過濾掉它們。另一種方法是稍微隨機化輸出。我們將在下一節討論輸出的隨機化。

### 溫度與機率 (Temperature and Probabilities)

在上一節中，您了解到 LLM 計算最可能的 token。但是如果您再剝開 LLM 這顆洋蔥的一層，事實證明，實際上，它在選擇單一 token 之前會計算所有可能 token 的機率。在底層選擇實際 token 的過程稱為採樣 (sampling)（見圖 2-11）。